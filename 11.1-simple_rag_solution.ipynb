{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "uiSPDftDJMWV",
        "bHrLhMMCJmvf",
        "_igGVJVT17cC"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "324c698571a444dcb6ec4580eaa0f270": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7623e1fefaac4f9ea13b176e3910581b",
              "IPY_MODEL_84553cb4990449c3b1df3a8821ade282",
              "IPY_MODEL_9053ac2f6d794232b223dd08fd9371e2"
            ],
            "layout": "IPY_MODEL_d53ef5e84e9848eaac18fc4686a061b2"
          }
        },
        "7623e1fefaac4f9ea13b176e3910581b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cb7ec4d859446a6bcd7bf26cf45124a",
            "placeholder": "​",
            "style": "IPY_MODEL_a5670c9337404859b8b73af4f848c482",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "84553cb4990449c3b1df3a8821ade282": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43713c8b2d3e4ac5bf077e3de0c86d98",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8fc75510f290495684af37a3877583a4",
            "value": 8
          }
        },
        "9053ac2f6d794232b223dd08fd9371e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f33046eb89ac4059a39028fbaa330785",
            "placeholder": "​",
            "style": "IPY_MODEL_39fdcb2576764d1c8775f84a274dbe2c",
            "value": " 8/8 [01:15&lt;00:00,  8.83s/it]"
          }
        },
        "d53ef5e84e9848eaac18fc4686a061b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cb7ec4d859446a6bcd7bf26cf45124a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5670c9337404859b8b73af4f848c482": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43713c8b2d3e4ac5bf077e3de0c86d98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fc75510f290495684af37a3877583a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f33046eb89ac4059a39028fbaa330785": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39fdcb2576764d1c8775f84a274dbe2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Kratak uvod u RAG koristeći LangChain i HuggingFace\n",
        "\n",
        "**Retrieval-augmented generation**, skraćeno RAG, je tehnika koja poboljšava pouzdanost generativnih modela sa mogućnostima pronalaženja (retrieval).\n",
        "\n",
        "RAG koristimo da dopunimo velike jezičke modele, skraćeno VJM tj. LLM - Large Language Model, tako što im na taj način omogućavamo pristup relevantnim informacijama iz spoljnih izvora, poboljšavajući time sposobnost VJM modela da generiše odgovore koji su relevantni i tačni za zadane upite.\n",
        "\n",
        "Pogledajte ilustraciju:\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1qRU-B8TjYDz8ANRB7Y1lPwIRdehijurL)\n",
        "\n",
        "**Izvor za ilustraciju:** [DataKolektiv](https://datakolektiv.com/)\n",
        "\n",
        "\n",
        "\n",
        "1. **Embeddings**  \n",
        "  *Embeddings* predstavlja vektorsku reprezentaciju, u našem slučaju, teksta.\n",
        "2. **Prompt + Query**  \n",
        "  *Prompt* predstavlja početni upit koju korisnik postavlja LLM modelu, a *query* je konkretno pitanje ili zahtjev koji se postavlja.\n",
        "3. **Query Embedding**  \n",
        "  *Query Embedding* je vektorska reprezentacija upita. Ovaj vektor se koristi za pretraživanje relevantnih dokumenata u vektorskoj reprezentaciji.\n",
        "4. **Relevant Context Documents**  \n",
        "  *Relevant Context Documents* su dokumenti ili informacije iz baze koje su relevantne za postavljeni upit.\n",
        "5. **Enhanced Prompt**  \n",
        "  *Enhanced Prompt* je verzija osnovnog prompta koja uključuje informacije iz relevantnih dokumenata, odnosno sadrži kontekst koji će LLM-u pomoći prilikom generisanja odgovora. Ovaj odgovor je rezultat RAG sistema koji kombinuje informacije iz početnog prompta, korisničkog upita i odgovarajućeg konteksta."
      ],
      "metadata": {
        "id": "95sKK-a27v-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zadatak\n",
        "\n",
        "Vaš zadatak je da kreirate RAG koristeći HuggingFace (za javno dostupne modele) i LangChain.\n",
        "\n",
        "**Koraci:**\n",
        "\n",
        "0. Instalacija potrebnih paketa\n",
        "1. Učitavanje i priprema podataka (dokumenata)\n",
        "2. Kreiranje vektorske baze i retrivera  \n",
        "3. Učitavanje LLM modela\n",
        "4. Definisanje pipeline-a\n",
        "\n",
        "\n",
        "**Napomene:**\n",
        "\n",
        "\n",
        "*   Koristite dokumentaciju:\n",
        "  * [HuggingFace Transformers](https://huggingface.co/docs/transformers/en/index)\n",
        "  * [HuggingFace Hub](https://huggingface.co/docs/hub/index)\n",
        "  * [LangChain](https://python.langchain.com/docs/get_started/introduction)\n",
        "*   Možete koristiti ChatGPT i Copilot (ex. Bing Chat) 🤖"
      ],
      "metadata": {
        "id": "oJPVoFSiFI7U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Instalacija potrebnih paketa"
      ],
      "metadata": {
        "id": "uiSPDftDJMWV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7T5L906l8sSH",
        "outputId": "86597801-8ba3-4523-a8c8-a5ee68b0eafa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.5/149.5 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.0/817.0 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.4/246.4 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q torch transformers accelerate bitsandbytes sentence-transformers faiss-gpu\n",
        "!pip install -q langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q arxiv\n",
        "!pip install -q pymupdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0D60YGkCkOdl",
        "outputId": "04d62691-0dcd-473f-feaf-2a7297fbcad1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/81.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Učitavanje i priprema podataka\n",
        "\n",
        "Koristite *LangChain* za učitavanje dokumenata ili nekog drugog tekstualnog sadržaja nad kojim želite izvršavati upite - [link](https://python.langchain.com/docs/integrations/document_loaders/).\n",
        "\n",
        "Kada odaberete podatke koje ćete koristiti, potrebno je da ih pripremite na odgovarajući način za dalju obradu i čuvanje u vektorskoj bazi.\n"
      ],
      "metadata": {
        "id": "oyc37Kd3JVyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import ArxivLoader\n",
        "\n",
        "document_id = \"1706.03762v1\" # inicijalna verzija rada Attention Is All You Need\n",
        "loader = ArxivLoader(query=document_id, load_max_docs=2)\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "AqQRkadsgEZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Published - datum kada je dokument objavljen ili posljednji put izmijenjen\n",
        "docs[0].metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wv2AoQKllc6G",
        "outputId": "d20a2f9b-b57b-44e9-f295-b6a251b92009"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Published': '2017-06-12',\n",
              " 'Title': 'Attention Is All You Need',\n",
              " 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
              " 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "doc_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=20)\n",
        "\n",
        "chunked_docs = doc_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "bYrlfcNDmRnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunked_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LHR6PT8nc_3",
        "outputId": "265e72d4-abeb-439d-959f-01501cbfd304"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "79"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Kreiranje vektorske baze i *retriever*-a\n",
        "\n",
        "Izaberite vekrorsku bazu koju ćete koristiti za čuvanje vektora (embedding-a) dokumenata, pregled podržanih baza možete pronaći na ovom [linku](https://python.langchain.com/docs/integrations/vectorstores/).\n",
        "\n",
        "Vektore trebate kreirati koristeći otvorene embedding modele, potražite neki manji embedding model na HuggingFace Hub-u, referišite se na [Massive Text Embedding Benchmark (MTEB) Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)."
      ],
      "metadata": {
        "id": "T6Bw_uXgJaWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores.utils import DistanceStrategy\n",
        "\n",
        "# ovdje smo koristili osnovni bge (BAAI general embedding) model\n",
        "# dimenzija vektora ovog modela je 768\n",
        "embeddings_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
        "VECTOR_DB = FAISS.from_documents(chunked_docs, embeddings_model, distance_strategy=DistanceStrategy.COSINE)"
      ],
      "metadata": {
        "id": "lxOC0Y2ZnoHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = VECTOR_DB.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
      ],
      "metadata": {
        "id": "M_7I_n4_oVIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Učitavanje LLM-a\n",
        "\n",
        "Potražite neki manji LLM model na HuggingFace Hub-u, referišite se na [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard). Cilj je da iskoristite model koji je optimizovan za brže izvršavanje.\n",
        "\n",
        "Prilikom instanciranja modela, možete koristiti konfiguracije koje optimizuju njegovo izvršavanje kao što je kvantizacija (quantization).\n"
      ],
      "metadata": {
        "id": "3pu8rAYIJfgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# ucitavamo optimizovanu tj. kvantizovanu verziju LLM modela, radi brzeg izvrsavanja\n",
        "# LLM model koji koristimo je zasnovan na Mistral modelu\n",
        "model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "llm_model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "324c698571a444dcb6ec4580eaa0f270",
            "7623e1fefaac4f9ea13b176e3910581b",
            "84553cb4990449c3b1df3a8821ade282",
            "9053ac2f6d794232b223dd08fd9371e2",
            "d53ef5e84e9848eaac18fc4686a061b2",
            "0cb7ec4d859446a6bcd7bf26cf45124a",
            "a5670c9337404859b8b73af4f848c482",
            "43713c8b2d3e4ac5bf077e3de0c86d98",
            "8fc75510f290495684af37a3877583a4",
            "f33046eb89ac4059a39028fbaa330785",
            "39fdcb2576764d1c8775f84a274dbe2c"
          ]
        },
        "id": "nY06nQfzor3X",
        "outputId": "b7beb960-af31-4be2-b66e-e71168444a13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "324c698571a444dcb6ec4580eaa0f270"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  Definisanje pipeline-a\n",
        "\n",
        "Koristite HuggingFace *transformers* biblioteku za konstruisanje lanca ili pipeline-a za generisanje teksta, ovaj korak uključuje postavljanje parametara kao što su prethodno definisani LLM model, tokenizator, temperatura i drugi.\n",
        "\n",
        "Potrebno je da definišete šablon za unos koristeći *PromptTemplate*, na ovaj način modelu dajete potrebne informacije o kontekstu i pitanju koje se postavlja.\n",
        "\n",
        "Na kraju je potrebno da povežete sve komponente lanca."
      ],
      "metadata": {
        "id": "bHrLhMMCJmvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from transformers import pipeline\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "text_generation_pipeline = pipeline(\n",
        "    model=llm_model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    temperature=0.2,\n",
        "    do_sample=True,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=True,\n",
        "    max_new_tokens=400,\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
        "\n",
        "# prompts\n",
        "\"\"\"\n",
        "#1\n",
        "Using the information contained in the context, give a comprehensive answer to\n",
        "the question. Respond only to the question asked, response should be concise and\n",
        " relevant to the question. Context is:\n",
        "\n",
        "#2\n",
        "Your task is to answer to the given question based on your knowledge. Use the following context:\n",
        "\n",
        "\"\"\"\n",
        "prompt_template = \"\"\"\n",
        "<|system|>\n",
        "Using the information contained in the context, give a comprehensive answer to\n",
        "the question. Respond only to the question asked, response should be concise and\n",
        " relevant to the question. Context is:\n",
        "{context}\n",
        "\n",
        "</s>\n",
        "<|user|>\n",
        "{question}\n",
        "</s>\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=prompt_template,\n",
        ")\n",
        "\n",
        "# povezivanje komponenata lanca\n",
        "llm_chain = prompt | llm | output_parser"
      ],
      "metadata": {
        "id": "pw96eAWdov9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "retriever = VECTOR_DB.as_retriever()\n",
        "\n",
        "components = {\n",
        "    \"context\": retriever,\n",
        "    \"question\": RunnablePassthrough()\n",
        "}\n",
        "\n",
        "rag_chain = components | llm_chain\n",
        "\n",
        "def invoke_rag_chain(question):\n",
        "  response = rag_chain.invoke(question)\n",
        "\n",
        "  return response"
      ],
      "metadata": {
        "id": "sVbI6nqkqp2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testiranje"
      ],
      "metadata": {
        "id": "_igGVJVT17cC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is the name of proposed architecture?\""
      ],
      "metadata": {
        "id": "rK42KG3wqvdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "invoke_rag_chain(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "_IzxBEB8sGk6",
        "outputId": "f873175a-0001-414a-ab4a-d0637e322289"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The name of the proposed architecture is the Transformer, as described in the paper \"Attention Is All You Need\" by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin (published on June 12, 2017). The Transformer is a simple network architecture based solely on attention mechanisms, which dispenses with recurrence and convolutions entirely and has shown superior performance in machine translation tasks compared to existing models.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is the main idea of attention?\""
      ],
      "metadata": {
        "id": "ABehZCCo0H5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "invoke_rag_chain(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "P_pyHosX0IvY",
        "outputId": "25463c36-e36f-4231-9365-0161ff5d8e2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In the context provided, the main idea of attention is a mechanism used in sequence transduction models, specifically in machine translation tasks, which allows the model to focus on specific parts of a sequence during processing. This mechanism, called self-attention, relates different positions of a single sequence to compute a representation of the sequence. It helps to improve the quality of the model\\'s output while making it more parallelizable and requiring less time to train compared to traditional sequence transduction models based on complex recurrent or convolutional neural networks. The authors of the paper \"Attention Is All You Need\" propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. The paper presents experimental results showing that their model outperforms existing best results in machine translation tasks, including ensembles, by significant margins. Overall, attention is a key component in modern sequence transduction models that enables them to process sequences more efficiently and accurately.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"How did the writers evaluate the significance of certain architectural elements?\""
      ],
      "metadata": {
        "id": "QNjyGyEX0S0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "invoke_rag_chain(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "mTRtY92z0U_l",
        "outputId": "4bd9eebd-18bf-4e11-ef56-39c6138cb960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The writers evaluated the significance of certain architectural elements through experiments and analysis. They varied the number of attention heads and the attention key and value dimensions, as well as the attention key size dk, and observed the impact on model quality. They also noted that some attention heads seemed to be involved in anaphora resolution, as evidenced by sharp attentions for specific words. Additionally, they found that bigger models were better and dropout was helpful in avoiding overfitting. These observations led them to suggest that determining compatibility may require a more sophisticated compatibility function than dot product. Overall, their experiments and analyses provided insights into the importance and effectiveness of various architectural elements in their proposed network architecture, the Transformer.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DPEf438XZLHF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}